# -*- coding: utf-8 -*-
"""M6 Ejercicio4 ErickSandoval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wTntXRwQOwd07hw1o8ITMAk59icRlHi2

# **Setup**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm

from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

data = pd.read_csv('https://raw.githubusercontent.com/Rwyld/Data-Science-Models/main/Modelos/PCA/HousePriceCSV.csv', sep = ';')

data = data.set_index('Id')
display(data.head(3))

"""# **Analisis Estadistico**"""

data.info()

data.isna().sum()

# Rellenando datos faltantes
data['LotFrontage'].fillna(data['LotFrontage'].mean(), inplace=True)
data['MasVnrArea'].fillna(data['MasVnrArea'].mean(), inplace=True)

data.describe()

ax = sns.histplot(data = data, x = data.SalePrice, stat = 'count', kde = True, alpha = 0.5)
ax.set_title('Sale price distribution', fontsize = 10, fontweight = "bold")
ax.tick_params(labelsize = 8)
ax.set_xlabel("")

"""# **Correlacion**"""

corr = data.corr()

fig, ax = plt.subplots(1,1, figsize = (15,10))

ax = sns.heatmap(corr, annot=True, cmap="YlGnBu")

"""Segun el analisis de correlacion, los datos cercanos a 1 (Colores cercanos a azul), son candidatos a estar correlacionadas entre si. Esto es porque tienen similitudes entre si, ya que podria representar una caracteristica en concreto, en este contexto, por ejemplo las variables relacionadas al subterraneo, tienen significados similares.

# **Modelo Regresion Linear**
"""

from datetime import datetime
start=datetime.now()

X = data.drop('SalePrice', axis = 1)
y = data['SalePrice']


modelo = LinearRegression()
modelo.fit(X, y)

print(datetime.now()-start)

"""# **Estandarizando**"""

# data = pd.DataFrame(StandardScaler().fit_transform(X))

pca_pipe = make_pipeline(StandardScaler(), PCA())
pca_pipe.fit(data)

modelo_pca = pca_pipe.named_steps['pca']

df = pd.DataFrame(
    data    = modelo_pca.components_,
    columns = data.columns,
    index   = [f"PC{num + 1}" for num in range(data.shape[1])]
)

df.head()

modelo_pca.explained_variance_ratio_

plt.plot(
    range(1, len(modelo_pca.components_) + 1),
    np.cumsum(modelo_pca.explained_variance_ratio_),
    marker = "o"
)
plt.xticks(
    ticks = np.arange(data.shape[1]) + 1,
)
plt.yticks(
    ticks  = np.linspace(0.55, 1, 10),
    labels = [f"{val:0.0%}" for val in np.linspace(0.55, 1, 10)]
);

plt.ylabel('Percent')
plt.xlabel('PCA')
plt.grid()

"""Se deberian escoger 11 PCA, para explicar un 80% de variabilidad, ya que es la suma acumulativa del porcentaje entre el PCA 1 hasta el PCA 11."""

newData = pca_pipe.transform(data)

proyecciones = pd.DataFrame(
    newData,
    columns = [f"PC{num + 1}" for num in range(data.shape[1])],
    index   = data.index
)
proyecciones.head(3)

modelData = proyecciones.iloc[:, 0:11]
modelData.head(3)

from datetime import datetime
start=datetime.now()

X = modelData

modelo = LinearRegression()
modelo.fit(X, y)

print(datetime.now()-start)

"""# **Interpretando**

Comparando ambos tiempos de demora obtenidos en los modelos, el que tardo menos tiempo en procesar, fue el que uso como variables predictoras los componentes principales en el modelo de Regresion Linear.

El usar componentes principales, se obtiene un procesamiento mas rapido y como su seleccion es un porcentaje de variabilidad alto, en este caso un 80%, se podria considerarse significativo para el conjunto de datos.
"""

