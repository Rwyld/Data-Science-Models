# -*- coding: utf-8 -*-
"""M6 Evaluacion ErickSandoval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kyR_uXlJ6o4k9VOSv4IcILY5F2UPyg54

# **Setup**
"""

#Liberias generales
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Liberias preprocesamiento
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

#Librerias modelos
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

data = pd.read_csv('https://raw.githubusercontent.com/Rwyld/Data-Science-Models/main/Modelos/KMeans/MktCampaignCSV%20-%20KMeans-PCA.csv')
data.head()

"""# **Analisis Exploratorio**"""

data.info()

data.describe()

corr = data.corr()

fig, ax = plt.subplots(1,1, figsize = (15,10))

ax = sns.heatmap(corr, annot=True, cmap="YlGnBu")

fig, ax = plt.subplots(2, 2, figsize = (10,6))

sns.histplot(data = data, x = data.Income, stat = 'count', kde = True, alpha = 0.5, ax = ax[0][0])
ax[0][0].set_title('Income distribution', fontsize = 10, fontweight = "bold")
ax[0][0].tick_params(labelsize = 8)
ax[0][0].set_xlabel("")

sns.histplot(data = data, x = data.Recency, stat = 'count', kde = True, alpha = 0.5, ax = ax[0][1])
ax[0][1].set_title('Recency distribution', fontsize = 10, fontweight = "bold")
ax[0][1].tick_params(labelsize = 8)
ax[0][1].set_xlabel("")

sns.histplot(data = data, x = data.Age, stat = 'count', kde = True, alpha = 0.5, ax = ax[1][0])
ax[1][0].set_title('Age distribution', fontsize = 10, fontweight = "bold")
ax[1][0].tick_params(labelsize = 8)
ax[1][0].set_xlabel("")

sns.histplot(data = data, x = data.Seniority, stat = 'count', kde = True, alpha = 0.5, ax = ax[1][1])
ax[1][1].set_title('Seniority distribution', fontsize = 10, fontweight = "bold")
ax[1][1].tick_params(labelsize = 8)
ax[1][1].set_xlabel("")

plt.tight_layout()
plt.show()

"""# **Detectando Anomalias**"""

dataPurchase = data[['NumDealsPurchases',	'NumWebPurchases',	'NumCatalogPurchases',	'NumStorePurchases', 'Age', 'Income']]

pca = PCA(n_components=2)
checkData = pca.fit_transform(dataPurchase)

isofor = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.05, random_state=42)
isofor.fit(checkData)

y_pred = isofor.predict(checkData)
anomalies = dataPurchase[y_pred == -1] 

print("Anomal√≠as detectadas")
display(anomalies.head(3))

print("\nNueva base de datos sin anomalias")
newData = data[y_pred == 1]
newData.head(3)

newData.describe()

"""# **Reduciendo data significativa**"""

newData = newData.drop('ID', axis=1)

modelData = StandardScaler().fit_transform(newData)
pcaModel = PCA().fit(modelData)

plt.plot(
    range(1, len(pcaModel.components_) + 1),
    np.cumsum(pcaModel.explained_variance_ratio_),
    marker = "o"
)
plt.xticks(
    ticks = np.arange(newData.shape[1]) + 1,
)
plt.yticks(
    ticks  = np.linspace(0.55, 1, 10),
    labels = [f"{val:0.0%}" for val in np.linspace(0.55, 1, 10)]
);

plt.ylabel('Percent')
plt.xlabel('PCA')
plt.grid()

pca = PCA(n_components=10)
pcaData = pca.fit_transform(newData)

scaler = StandardScaler()
scalerData = scaler.fit_transform(pcaData)

unscalerData = scaler.inverse_transform(scalerData)
originalData = pca.inverse_transform(unscalerData)

newModelData = pd.DataFrame(originalData, columns = newData.columns)
newModelData.head(3)

"""# **Modelando Clusters**"""

kmeans_kwargs = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,
    "random_state": 1234,
}

# Una lista contiene los valores de SSE para cada k
sse = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs) # el operador (**) es de desempaquetado del diccionario de Python 
    kmeans.fit(newModelData)
    sse.append(kmeans.inertia_)

plt.style.use("fivethirtyeight")
 plt.plot(range(1, 10), sse)
 plt.xticks(range(1, 10))
 plt.xlabel("Number of Clusters")
 plt.ylabel("SSE")
 plt.show()

kmeans = KMeans(
    init="random",
    n_clusters=3,
    n_init=10,
    max_iter=300,
    random_state=1234
)
kmeans.fit(newModelData)

centroids = pd.DataFrame(kmeans.cluster_centers_, columns=newModelData.columns).sort_values(by = ['Income'], ascending = True).reset_index().drop(['index'], axis=1)

clusterData = pd.DataFrame(newModelData)
clusterData['cluster'] = kmeans.labels_
clusterData[clusterData['cluster'] == 1].head(3)

"""# **Visualizacion Clusters**"""

sns.scatterplot(x=clusterData.Income, y=clusterData.Recency, data=clusterData, hue = clusterData.cluster)

display(centroids)

"""# **Interpretando**

Los centroides nos indican el punto medio o el centro de cada cluster.

Por ejemplo, tomando en cuenta la variable Income:

- En el cluster 0 su centroide es de 29112
- En el cluster 1 su centroide es de 51642
- En el cluster 2 su centroide es de 74562

Y asi se observa para cada variable su respectivo centroide en su correspondiente Cluster. 
Por lo tanto, los centroides nos ayudan a predecir nuevos datos cercanos a los cluster mediante el calculo de la distancia euclidiana entre ellos.
"""